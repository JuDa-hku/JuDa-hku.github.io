---
layout: post
title: KMeans
categories:
- Machine Learning
tags:
- Algorithm
- Scikit
---
* KMeans
* A Real Case

---

## KMeans with plot
KMeans method is a very basic way to classify data, here I show a easy example to show how to use it to classify 2-d data with 3 labels.

{% highlight python %}
import numpy as np
import pylab as pl
from matplotlib.colors import ListedColormap
##useful tool to have a colormap
from sklearn import neighbors

class1 = np.random.rand(30,2)
label1 = np.array([np.zeros(30)]).T
class1 = np.concatenate((class1, label1), axis=1)
class2 = np.random.rand(30,2)
label2 = np.array([np.ones(30)]).T
class2 = np.concatenate((class2, label2), axis=1)
class3 = np.random.rand(30,2)
label3 = np.array([np.ones(30)+1]).T
class3 = np.concatenate((class3, label3), axis=1)
class1[:,1] = class1[:,1] + 1
class2[:,0] = class2[:,0] + 0.5
dataset = np.concatenate((class1, class2, class3))
X = dataset[:, :-1]
y = dataset[:, -1]


cmap_data = ListedColormap([(1,0.6,0.6), (0.6, 1, 0.6), (0.6, 0.6, 1)])

h = 0.02 #step size in the mesh
n_neighbors = (1, 5, 10)
pl.figure(figsize = (8, 4)) 
for n_neighbor, i in zip(n_neighbors, range(1,4)):
    clf = neighbors.KNeighborsClassifier(n_neighbor)
    clf.fit(X, y)
    x_min, x_max = X[:,0].min()*0.9, X[:,0].max()*1.1
    y_min, y_max = X[:,1].min()*0.9, X[:,1].max()*1.1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
   
    ax = pl.subplot(1,3,i)
    ax.set_title('K = %i' %n_neighbor)
    pl.pcolormesh(xx, yy, Z, cmap= cmap_data)
    pl.xlim(x_min, x_max)
    pl.ylim(y_min, y_max)
    pl.xticks([])
    pl.yticks([])
    pl.scatter(class1[:,0],class1[:,1], c='r', marker = 'o')
    pl.scatter(class2[:,0],class2[:,1], c='g', marker = 'o')
    pl.scatter(class3[:,0],class3[:,1], c='b', marker = 'o')
    
pl.savefig('knn_plot.png')
{% endhighlight python %}

The plot is given below

![knn_plot](/png/knn_plot.png?raw=true)

## A real case
We use KNN classifier to analysis iris data set, and plot. The **curse of dimensionality** means that for an estimator to be effective, you need distance less than value d. You need more than 1/d points. For high dimensional data, you required 1/d^p points.
{%highlight python %}
from sklearn import datasets
from sklearn.neighbors import KNeighborsClassifier
from matplotlib.colors import ListedColormap
import numpy as np
import pylab as pl
iris = datasets.load_iris()
iris_X = iris.data
iris_X = iris_X[:,0:2]
iris_y = iris.target
#sepal length speal width
#petal length petal width
np.random.seed(0)
indices = np.random.permutation(len(iris_X))
iris_X_train = iris_X[indices[:-10]]
iris_y_train = iris_y[indices[:-10]]
iris_X_test = iris_X[indices[-10:]]
iris_y_test = iris_y[indices[-10:]]
knn = KNeighborsClassifier()
knn.fit(iris_X_train, iris_y_train)
cmap_data = ListedColormap([(1,0.6,0.6),(0.6,1,0.6),(0.6,0.6,1)])
h = 0.02
x_min, x_max = iris_X[:,0].min()*0.9, iris_X[:,0].max()*1.1
y_min, y_max = iris_X[:,1].min()*0.9, iris_X[:,1].max()*1.1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max,h))
Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
pl.figure(figsize=(6,4))
pl.pcolormesh(xx, yy, Z, cmap=cmap_data)
pl.xlabel('sepal length')
pl.ylabel('petal width')
#pl.xticks([])

pl.xlim(x_min, x_max)
pl.ylim(y_min, y_max)
t0 = [i==0 for i in iris_y_train]
t0 = t0*np.arange(len(iris_y_train))
t1 = [i==1 for i in iris_y_train]
t1 = t1*np.arange(len(iris_y_train))
t2 = [i==2 for i in iris_y_train]
t2 = t2*np.arange(len(iris_y_train))
pl.scatter(iris_X_train[t0,0], iris_X_train[t0,1], c='r')
pl.scatter(iris_X_train[t1,0], iris_X_train[t1,1], c='g')
pl.scatter(iris_X_train[t2,0], iris_X_train[t2,1], c='b')
pl.savefig('knn_real_case.png')
{%endhighlight python%}
![knn_real_case](/png/knn_real_case.png)

